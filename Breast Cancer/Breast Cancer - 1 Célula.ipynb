{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from auto_ml import Predictor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = ['ID', 'Target']\n",
    "cell_1 = ['Radius-1', 'Texture-1', 'Perimeter-1', 'Area-1', 'Smoothness-1', 'Compactness-1', 'Concavity-1', 'Concave Points-1', 'Symmetry-1', 'Fractal Dimension-1']\n",
    "cell_2 = ['Radius-2', 'Texture-2', 'Perimeter-2', 'Area-2', 'Smoothness-2', 'Compactness-2', 'Concavity-2', 'Concave Points-2', 'Symmetry-2', 'Fractal Dimension-2']\n",
    "cell_3 = ['Radius-3', 'Texture-3', 'Perimeter-3', 'Area-3', 'Smoothness-3', 'Compactness-3', 'Concavity-3', 'Concave Points-3', 'Symmetry-3', 'Fractal Dimension-3']\n",
    "\n",
    "header = head + cell_1 + cell_2 + cell_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Target</th>\n",
       "      <th>Radius-1</th>\n",
       "      <th>Texture-1</th>\n",
       "      <th>Perimeter-1</th>\n",
       "      <th>Area-1</th>\n",
       "      <th>Smoothness-1</th>\n",
       "      <th>Compactness-1</th>\n",
       "      <th>Concavity-1</th>\n",
       "      <th>Concave Points-1</th>\n",
       "      <th>...</th>\n",
       "      <th>Radius-3</th>\n",
       "      <th>Texture-3</th>\n",
       "      <th>Perimeter-3</th>\n",
       "      <th>Area-3</th>\n",
       "      <th>Smoothness-3</th>\n",
       "      <th>Compactness-3</th>\n",
       "      <th>Concavity-3</th>\n",
       "      <th>Concave Points-3</th>\n",
       "      <th>Symmetry-3</th>\n",
       "      <th>Fractal Dimension-3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID Target  Radius-1  Texture-1  Perimeter-1  Area-1  Smoothness-1  \\\n",
       "0    842302      M     17.99      10.38       122.80  1001.0       0.11840   \n",
       "1    842517      M     20.57      17.77       132.90  1326.0       0.08474   \n",
       "2  84300903      M     19.69      21.25       130.00  1203.0       0.10960   \n",
       "3  84348301      M     11.42      20.38        77.58   386.1       0.14250   \n",
       "4  84358402      M     20.29      14.34       135.10  1297.0       0.10030   \n",
       "\n",
       "   Compactness-1  Concavity-1  Concave Points-1  ...  Radius-3  Texture-3  \\\n",
       "0        0.27760       0.3001           0.14710  ...     25.38      17.33   \n",
       "1        0.07864       0.0869           0.07017  ...     24.99      23.41   \n",
       "2        0.15990       0.1974           0.12790  ...     23.57      25.53   \n",
       "3        0.28390       0.2414           0.10520  ...     14.91      26.50   \n",
       "4        0.13280       0.1980           0.10430  ...     22.54      16.67   \n",
       "\n",
       "   Perimeter-3  Area-3  Smoothness-3  Compactness-3  Concavity-3  \\\n",
       "0       184.60  2019.0        0.1622         0.6656       0.7119   \n",
       "1       158.80  1956.0        0.1238         0.1866       0.2416   \n",
       "2       152.50  1709.0        0.1444         0.4245       0.4504   \n",
       "3        98.87   567.7        0.2098         0.8663       0.6869   \n",
       "4       152.20  1575.0        0.1374         0.2050       0.4000   \n",
       "\n",
       "   Concave Points-3  Symmetry-3  Fractal Dimension-3  \n",
       "0            0.2654      0.4601              0.11890  \n",
       "1            0.1860      0.2750              0.08902  \n",
       "2            0.2430      0.3613              0.08758  \n",
       "3            0.2575      0.6638              0.17300  \n",
       "4            0.1625      0.2364              0.07678  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('wdbc.data', names=header)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Target'] = data['Target'].map({'M': 1, 'B': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpeza da Base de Dados - 1 Célula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(head + cell_3 + cell_2, axis = 1)\n",
    "y = data['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "scale_X_train = scaler.transform(X_train)\n",
    "scale_X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métodos de Classificação - 1 Célula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "_1_cell = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solver: newton-cg\n",
      "Accuracy Score: 0.950\n",
      "\n",
      "Solver: lbfgs\n",
      "Accuracy Score: 0.950\n",
      "\n",
      "Solver: liblinear\n",
      "Accuracy Score: 0.945\n",
      "\n",
      "Solver: sag\n",
      "Accuracy Score: 0.950\n",
      "\n",
      "Solver: saga\n",
      "Accuracy Score: 0.950\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucas/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "solvers = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "\n",
    "lr_X_train = scale_X_train\n",
    "lr_X_test = scale_X_test\n",
    "lr_y_train = y_train\n",
    "lr_y_test = y_test\n",
    "\n",
    "_1_cell['lr'] = {'score': 0, 'solver': ''}\n",
    "\n",
    "for solver in solvers:\n",
    "    lr = LogisticRegression(solver= solver, multi_class= 'ovr')\n",
    "    lr.fit(lr_X_train, lr_y_train)\n",
    "    lr_pred = lr.predict(lr_X_test)\n",
    "    score = lr.score(lr_X_train, lr_y_train)\n",
    "    print ('Solver: ' + solver)\n",
    "    print('Accuracy Score: {0:.3f}\\n'.format(score))\n",
    "    if score > _1_cell['lr']['score']:\n",
    "        _1_cell['lr']['solver'] = solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucas/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(solver= _1_cell['lr']['solver'], multi_class= 'ovr')\n",
    "lr.fit(lr_X_train, lr_y_train)\n",
    "lr_pred = lr.predict(lr_X_test)\n",
    "\n",
    "score = lr.score(lr_X_test, lr_y_test)\n",
    "_1_cell['lr']['score'] = score\n",
    "_1_cell['lr']['cr'] = classification_report(lr_y_test, lr_pred)\n",
    "_1_cell['lr']['ps'] = precision_score(lr_y_test, lr_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.910\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nbc_X_train = scale_X_train\n",
    "nbc_X_test = scale_X_test\n",
    "nbc_y_train = y_train\n",
    "nbc_y_test = y_test\n",
    "\n",
    "_1_cell['nbc'] = {'score': 0}\n",
    "\n",
    "nbc = GaussianNB()\n",
    "nbc.fit(nbc_X_train, nbc_y_train)\n",
    "nbc_pred = nbc.predict(nbc_X_test)\n",
    "score = nbc.score(nbc_X_train, nbc_y_train)\n",
    "print('Accuracy Score: {0:.3f}'.format(score))\n",
    "_1_cell['nbc']['score'] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "score = nbc.score(nbc_X_test, nbc_y_test)\n",
    "_1_cell['nbc']['score'] = score\n",
    "_1_cell['nbc']['cr'] = classification_report(nbc_y_test, nbc_pred)\n",
    "_1_cell['nbc']['ps'] = precision_score(nbc_y_test, nbc_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#knn_error_rate = []\n",
    "knn_score = []\n",
    "\n",
    "knn_X_train = scale_X_train\n",
    "knn_X_test = scale_X_test\n",
    "knn_y_train = y_train\n",
    "knn_y_test = y_test\n",
    "\n",
    "_1_cell['knn'] = {'score': 0, 'k': 0}\n",
    "\n",
    "for i in range(1, 30):\n",
    "    knn = KNeighborsClassifier(n_neighbors = i)\n",
    "    knn.fit(knn_X_train, knn_y_train)\n",
    "    knn_pred = knn.predict(knn_X_test)\n",
    "    score = knn.score(knn_X_train, knn_y_train)\n",
    "    #knn_error_rate.append(np.mean(knn_pred != knn_y_test))\n",
    "    knn_score.append(score)\n",
    "    if score > _1_cell['knn']['score']:\n",
    "        _1_cell['knn']['k'] = i\n",
    "        _1_cell['knn']['score'] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-93699a4e5d55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m plt.plot(range(1, 30), knn_score, color = 'black', linestyle = 'dashed', \n\u001b[1;32m      3\u001b[0m          marker = 'o', markerfacecolor = 'blue', markersize = 10)\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy Score vs K Value'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'K'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize = (10, 6))\n",
    "plt.plot(range(1, 30), knn_score, color = 'black', linestyle = 'dashed', \n",
    "         marker = 'o', markerfacecolor = 'blue', markersize = 10)\n",
    "plt.title('Accuracy Score vs K Value')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Accuracy Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('K: ' + str(_1_cell['knn']['k']))\n",
    "print('Accuracy Score: {0:.3f}\\n'.format(_1_cell['knn']['score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = _1_cell['knn']['k'])\n",
    "knn.fit(knn_X_train, knn_y_train)\n",
    "knn_pred = knn.predict(knn_X_test)\n",
    "\n",
    "score = knn.score(knn_X_test, knn_y_test)\n",
    "_1_cell['knn']['score'] = score\n",
    "_1_cell['knn']['cr'] = classification_report(knn_y_test, knn_pred)\n",
    "_1_cell['knn']['ps'] = precision_score(knn_y_test, knn_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svm_X_train = scale_X_train\n",
    "svm_X_test = scale_X_test\n",
    "svm_y_train = y_train\n",
    "svm_y_test = y_test\n",
    "\n",
    "_1_cell['svm'] = {'score': 0, 'kernel': '0'}\n",
    "\n",
    "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "\n",
    "for kernel in kernels:\n",
    "    svm = SVC(kernel = kernel, random_state = 11)\n",
    "    svm.fit(svm_X_train, svm_y_train)\n",
    "    svm_pred = svm.predict(svm_X_test)\n",
    "    score = svm.score(svm_X_train, svm_y_train)\n",
    "    print ('Kernel: ' + kernel)\n",
    "    print('Accuracy Score: {0:.3f}\\n'.format(score))\n",
    "    if score > _1_cell['svm']['score']:\n",
    "        _1_cell['svm']['score'] = score\n",
    "        _1_cell['svm']['kernel'] = kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svm = SVC(kernel = _1_cell['svm']['kernel'], random_state = 11)\n",
    "svm.fit(svm_X_train, svm_y_train)\n",
    "svm_pred = svm.predict(svm_X_test)\n",
    "\n",
    "score = svm.score(svm_X_test, svm_y_test)\n",
    "_1_cell['svm']['score'] = score\n",
    "_1_cell['svm']['cr'] = classification_report(svm_y_test, svm_pred)\n",
    "_1_cell['svm']['ps'] = precision_score(svm_y_test, svm_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc_X_train = scale_X_train\n",
    "rfc_X_test = scale_X_test\n",
    "rfc_y_train = y_train\n",
    "rfc_y_test = y_test\n",
    "\n",
    "_1_cell['rfc'] = {'score': 0, 'trees': 0, 'leafs': 0}\n",
    "\n",
    "for i in range(1, 1000, 50):\n",
    "    for j in range(1, 10):\n",
    "        rfc = RandomForestClassifier(n_estimators = i, min_samples_leaf = j, random_state = 11)\n",
    "        rfc.fit(rfc_X_train, rfc_y_train)\n",
    "        rfc_pred = rfc.predict(rfc_X_test)\n",
    "        score = rfc.score(rfc_X_train, rfc_y_train)\n",
    "        if _1_cell['rfc']['score'] < score:\n",
    "            _1_cell['rfc']['score'] = score\n",
    "            _1_cell['rfc']['trees'] = i\n",
    "            _1_cell['rfc']['leafs'] = j\n",
    "            print('Number of trees: %.2f' % i)\n",
    "            print('Minimum sample leaf: %.2f' % j)\n",
    "            print ('Accuracy Score: %.2f\\n' % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators = _1_cell['rfc']['trees'], min_samples_leaf = _1_cell['rfc']['leafs'], random_state = 11)\n",
    "rfc.fit(rfc_X_train, rfc_y_train)\n",
    "rfc_pred = rfc.predict(rfc_X_test)\n",
    "\n",
    "score = rfc.score(rfc_X_test, rfc_y_test)\n",
    "_1_cell['rfc']['score'] = score\n",
    "_1_cell['rfc']['cr'] = classification_report(rfc_y_test, rfc_pred)\n",
    "_1_cell['rfc']['ps'] = precision_score(rfc_y_test, rfc_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exportando os Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('1-Célula.txt', 'w') as outfile:\n",
    "    json.dump(_1_cell, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to auto_ml! We're about to go through and make sense of your data using machine learning, and give you a production-ready pipeline to get predictions with.\n",
      "\n",
      "If you have any issues, or new feature ideas, let us know at http://auto.ml\n",
      "You are running on version 2.9.10\n",
      "Now using the model training_params that you passed in:\n",
      "{}\n",
      "After overwriting our defaults with your values, here are the final params that will be used to initialize the model:\n",
      "{'presort': False, 'learning_rate': 0.1, 'warm_start': True}\n",
      "Running basic data cleaning\n",
      "Fitting DataFrameVectorizer\n",
      "Now using the model training_params that you passed in:\n",
      "{}\n",
      "After overwriting our defaults with your values, here are the final params that will be used to initialize the model:\n",
      "{'presort': False, 'learning_rate': 0.1, 'warm_start': True}\n",
      "\n",
      "\n",
      "********************************************************************************************\n",
      "About to fit the pipeline for the model GradientBoostingClassifier to predict Target\n",
      "Started at:\n",
      "2020-01-23 20:53:21\n",
      "[1] random_holdout_set_from_training_data's score is: -0.193\n",
      "[2] random_holdout_set_from_training_data's score is: -0.169\n",
      "[3] random_holdout_set_from_training_data's score is: -0.15\n",
      "[4] random_holdout_set_from_training_data's score is: -0.133\n",
      "[5] random_holdout_set_from_training_data's score is: -0.122\n",
      "[6] random_holdout_set_from_training_data's score is: -0.111\n",
      "[7] random_holdout_set_from_training_data's score is: -0.104\n",
      "[8] random_holdout_set_from_training_data's score is: -0.097\n",
      "[9] random_holdout_set_from_training_data's score is: -0.092\n",
      "[10] random_holdout_set_from_training_data's score is: -0.088\n",
      "[11] random_holdout_set_from_training_data's score is: -0.083\n",
      "[12] random_holdout_set_from_training_data's score is: -0.082\n",
      "[13] random_holdout_set_from_training_data's score is: -0.081\n",
      "[14] random_holdout_set_from_training_data's score is: -0.078\n",
      "[15] random_holdout_set_from_training_data's score is: -0.076\n",
      "[16] random_holdout_set_from_training_data's score is: -0.076\n",
      "[17] random_holdout_set_from_training_data's score is: -0.074\n",
      "[18] random_holdout_set_from_training_data's score is: -0.073\n",
      "[19] random_holdout_set_from_training_data's score is: -0.073\n",
      "[20] random_holdout_set_from_training_data's score is: -0.074\n",
      "[21] random_holdout_set_from_training_data's score is: -0.074\n",
      "[22] random_holdout_set_from_training_data's score is: -0.073\n",
      "[23] random_holdout_set_from_training_data's score is: -0.073\n",
      "[24] random_holdout_set_from_training_data's score is: -0.073\n",
      "[25] random_holdout_set_from_training_data's score is: -0.073\n",
      "[26] random_holdout_set_from_training_data's score is: -0.071\n",
      "[27] random_holdout_set_from_training_data's score is: -0.07\n",
      "[28] random_holdout_set_from_training_data's score is: -0.07\n",
      "[29] random_holdout_set_from_training_data's score is: -0.069\n",
      "[30] random_holdout_set_from_training_data's score is: -0.069\n",
      "[31] random_holdout_set_from_training_data's score is: -0.07\n",
      "[32] random_holdout_set_from_training_data's score is: -0.069\n",
      "[33] random_holdout_set_from_training_data's score is: -0.067\n",
      "[34] random_holdout_set_from_training_data's score is: -0.067\n",
      "[35] random_holdout_set_from_training_data's score is: -0.068\n",
      "[36] random_holdout_set_from_training_data's score is: -0.068\n",
      "[37] random_holdout_set_from_training_data's score is: -0.067\n",
      "[38] random_holdout_set_from_training_data's score is: -0.068\n",
      "[39] random_holdout_set_from_training_data's score is: -0.068\n",
      "[40] random_holdout_set_from_training_data's score is: -0.069\n",
      "[41] random_holdout_set_from_training_data's score is: -0.069\n",
      "[42] random_holdout_set_from_training_data's score is: -0.068\n",
      "[43] random_holdout_set_from_training_data's score is: -0.067\n",
      "[44] random_holdout_set_from_training_data's score is: -0.068\n",
      "[45] random_holdout_set_from_training_data's score is: -0.069\n",
      "[46] random_holdout_set_from_training_data's score is: -0.069\n",
      "[47] random_holdout_set_from_training_data's score is: -0.069\n",
      "[48] random_holdout_set_from_training_data's score is: -0.068\n",
      "[49] random_holdout_set_from_training_data's score is: -0.069\n",
      "[50] random_holdout_set_from_training_data's score is: -0.07\n",
      "[52] random_holdout_set_from_training_data's score is: -0.069\n",
      "[54] random_holdout_set_from_training_data's score is: -0.068\n",
      "[56] random_holdout_set_from_training_data's score is: -0.068\n",
      "[58] random_holdout_set_from_training_data's score is: -0.066\n",
      "[60] random_holdout_set_from_training_data's score is: -0.065\n",
      "[62] random_holdout_set_from_training_data's score is: -0.064\n",
      "[64] random_holdout_set_from_training_data's score is: -0.064\n",
      "[66] random_holdout_set_from_training_data's score is: -0.064\n",
      "[68] random_holdout_set_from_training_data's score is: -0.062\n",
      "[70] random_holdout_set_from_training_data's score is: -0.062\n",
      "[72] random_holdout_set_from_training_data's score is: -0.059\n",
      "[74] random_holdout_set_from_training_data's score is: -0.058\n",
      "[76] random_holdout_set_from_training_data's score is: -0.06\n",
      "[78] random_holdout_set_from_training_data's score is: -0.059\n",
      "[80] random_holdout_set_from_training_data's score is: -0.059\n",
      "[82] random_holdout_set_from_training_data's score is: -0.059\n",
      "[84] random_holdout_set_from_training_data's score is: -0.058\n",
      "[86] random_holdout_set_from_training_data's score is: -0.058\n",
      "[88] random_holdout_set_from_training_data's score is: -0.056\n",
      "[90] random_holdout_set_from_training_data's score is: -0.056\n",
      "[92] random_holdout_set_from_training_data's score is: -0.055\n",
      "[94] random_holdout_set_from_training_data's score is: -0.055\n",
      "[96] random_holdout_set_from_training_data's score is: -0.056\n",
      "[98] random_holdout_set_from_training_data's score is: -0.055\n",
      "[100] random_holdout_set_from_training_data's score is: -0.055\n",
      "[103] random_holdout_set_from_training_data's score is: -0.054\n",
      "[106] random_holdout_set_from_training_data's score is: -0.053\n",
      "[109] random_holdout_set_from_training_data's score is: -0.052\n",
      "[112] random_holdout_set_from_training_data's score is: -0.052\n",
      "[115] random_holdout_set_from_training_data's score is: -0.052\n",
      "[118] random_holdout_set_from_training_data's score is: -0.051\n",
      "[121] random_holdout_set_from_training_data's score is: -0.051\n",
      "[124] random_holdout_set_from_training_data's score is: -0.051\n",
      "[127] random_holdout_set_from_training_data's score is: -0.051\n",
      "[130] random_holdout_set_from_training_data's score is: -0.051\n",
      "[133] random_holdout_set_from_training_data's score is: -0.051\n",
      "[136] random_holdout_set_from_training_data's score is: -0.051\n",
      "[139] random_holdout_set_from_training_data's score is: -0.05\n",
      "[142] random_holdout_set_from_training_data's score is: -0.051\n",
      "[145] random_holdout_set_from_training_data's score is: -0.05\n",
      "[148] random_holdout_set_from_training_data's score is: -0.049\n",
      "[151] random_holdout_set_from_training_data's score is: -0.049\n",
      "[154] random_holdout_set_from_training_data's score is: -0.049\n",
      "[157] random_holdout_set_from_training_data's score is: -0.049\n",
      "[160] random_holdout_set_from_training_data's score is: -0.049\n",
      "[163] random_holdout_set_from_training_data's score is: -0.049\n",
      "[166] random_holdout_set_from_training_data's score is: -0.049\n",
      "[169] random_holdout_set_from_training_data's score is: -0.049\n",
      "[172] random_holdout_set_from_training_data's score is: -0.049\n",
      "[175] random_holdout_set_from_training_data's score is: -0.049\n",
      "[178] random_holdout_set_from_training_data's score is: -0.049\n",
      "[181] random_holdout_set_from_training_data's score is: -0.049\n",
      "[184] random_holdout_set_from_training_data's score is: -0.049\n",
      "[187] random_holdout_set_from_training_data's score is: -0.049\n",
      "[190] random_holdout_set_from_training_data's score is: -0.049\n",
      "[193] random_holdout_set_from_training_data's score is: -0.049\n",
      "[196] random_holdout_set_from_training_data's score is: -0.049\n",
      "[199] random_holdout_set_from_training_data's score is: -0.049\n",
      "[202] random_holdout_set_from_training_data's score is: -0.049\n",
      "[205] random_holdout_set_from_training_data's score is: -0.049\n",
      "[208] random_holdout_set_from_training_data's score is: -0.049\n",
      "The number of estimators that were the best for this training dataset: 148\n",
      "The best score on the holdout set: -0.0492206194693028\n",
      "Finished training the pipeline!\n",
      "Total training time:\n",
      "0:00:01\n",
      "\n",
      "\n",
      "Here are the results from our GradientBoostingClassifier\n",
      "predicting Target\n",
      "Calculating feature responses, for advanced analytics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The printed list will only contain at most the top 100 features.\n",
      "+----+---------------------+--------------+----------+-------------------+-------------------+-----------+-----------+-----------+-----------+\n",
      "|    | Feature Name        |   Importance |    Delta |   FR_Decrementing |   FR_Incrementing |   FRD_abs |   FRI_abs |   FRD_MAD |   FRI_MAD |\n",
      "|----+---------------------+--------------+----------+-------------------+-------------------+-----------+-----------+-----------+-----------|\n",
      "|  8 | Symmetry-1          |       0.0000 |   0.0136 |           -0.0001 |            0.0000 |    0.0001 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 25 | Compactness-3       |       0.0000 |   0.0803 |           -0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "|  9 | Fractal Dimension-1 |       0.0001 |   0.0037 |           -0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 17 | Concave Points-2    |       0.0001 |   0.0032 |            0.0001 |           -0.0000 |    0.0001 |    0.0001 |    0.0000 |    0.0000 |\n",
      "| 11 | Texture-2           |       0.0002 |   0.2792 |            0.0000 |           -0.0000 |    0.0001 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 29 | Fractal Dimension-3 |       0.0004 |   0.0090 |            0.0005 |           -0.0000 |    0.0005 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 12 | Perimeter-2         |       0.0004 |   1.0428 |           -0.0003 |            0.0001 |    0.0006 |    0.0005 |    0.0000 |    0.0000 |\n",
      "| 16 | Concavity-2         |       0.0008 |   0.0165 |            0.0001 |           -0.0001 |    0.0001 |    0.0002 |    0.0000 |    0.0000 |\n",
      "|  3 | Area-1              |       0.0009 | 178.6978 |            0.0007 |            0.0008 |    0.0022 |    0.0009 |    0.0000 |    0.0000 |\n",
      "| 15 | Compactness-2       |       0.0010 |   0.0094 |           -0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "|  0 | Radius-1            |       0.0010 |   1.7961 |            0.0017 |           -0.0006 |    0.0020 |    0.0011 |    0.0000 |    0.0000 |\n",
      "| 18 | Symmetry-2          |       0.0010 |   0.0042 |            0.0003 |            0.0001 |    0.0003 |    0.0001 |    0.0000 |    0.0000 |\n",
      "| 14 | Smoothness-2        |       0.0010 |   0.0016 |            0.0004 |           -0.0001 |    0.0006 |    0.0002 |    0.0000 |    0.0000 |\n",
      "| 19 | Fractal Dimension-2 |       0.0011 |   0.0014 |            0.0008 |           -0.0000 |    0.0008 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 28 | Symmetry-3          |       0.0015 |   0.0313 |           -0.0000 |            0.0001 |    0.0003 |    0.0002 |    0.0000 |    0.0000 |\n",
      "|  6 | Concavity-1         |       0.0016 |   0.0407 |            0.0000 |            0.0002 |    0.0003 |    0.0002 |    0.0000 |    0.0000 |\n",
      "|  2 | Perimeter-1         |       0.0018 |  12.3711 |            0.0002 |           -0.0002 |    0.0004 |    0.0002 |    0.0000 |    0.0000 |\n",
      "|  4 | Smoothness-1        |       0.0020 |   0.0071 |           -0.0000 |           -0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "|  5 | Compactness-1       |       0.0023 |   0.0276 |            0.0009 |           -0.0000 |    0.0009 |    0.0000 |    0.0000 |    0.0000 |\n",
      "|  7 | Concave Points-1    |       0.0023 |   0.0197 |           -0.0008 |            0.0016 |    0.0016 |    0.0018 |    0.0000 |    0.0000 |\n",
      "| 26 | Concavity-3         |       0.0038 |   0.1061 |            0.0003 |           -0.0002 |    0.0003 |    0.0004 |    0.0000 |    0.0000 |\n",
      "| 20 | Radius-3            |       0.0038 |   2.4589 |           -0.0021 |            0.0015 |    0.0021 |    0.0017 |    0.0000 |    0.0000 |\n",
      "| 10 | Radius-2            |       0.0040 |   0.1412 |            0.0000 |           -0.0000 |    0.0001 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 13 | Area-2              |       0.0101 |  22.1091 |           -0.0003 |            0.0021 |    0.0003 |    0.0022 |    0.0000 |    0.0000 |\n",
      "| 24 | Smoothness-3        |       0.0197 |   0.0114 |           -0.0043 |            0.0026 |    0.0043 |    0.0026 |    0.0000 |    0.0000 |\n",
      "|  1 | Texture-1           |       0.0240 |   2.1463 |           -0.0027 |            0.0017 |    0.0028 |    0.0021 |    0.0000 |    0.0000 |\n",
      "| 21 | Texture-3           |       0.0420 |   3.0455 |           -0.0037 |            0.0032 |    0.0037 |    0.0032 |    0.0000 |    0.0000 |\n",
      "| 27 | Concave Points-3    |       0.0938 |   0.0331 |           -0.0084 |            0.0072 |    0.0084 |    0.0072 |    0.0000 |    0.0000 |\n",
      "| 23 | Area-3              |       0.3742 | 287.7963 |           -0.0302 |            0.0536 |    0.0303 |    0.0536 |    0.0000 |    0.0001 |\n",
      "| 22 | Perimeter-3         |       0.4050 |  17.1585 |           -0.0091 |            0.0249 |    0.0092 |    0.0250 |    0.0000 |    0.0000 |\n",
      "+----+---------------------+--------------+----------+-------------------+-------------------+-----------+-----------+-----------+-----------+\n",
      "\n",
      "\n",
      "*******\n",
      "Legend:\n",
      "Importance = Feature Importance\n",
      "     Explanation: A weighted measure of how much of the variance the model is able to explain is due to this column\n",
      "FR_delta = Feature Response Delta Amount\n",
      "     Explanation: Amount this column was incremented or decremented by to calculate the feature reponses\n",
      "FR_Decrementing = Feature Response From Decrementing Values In This Column By One FR_delta\n",
      "     Explanation: Represents how much the predicted output values respond to subtracting one FR_delta amount from every value in this column\n",
      "FR_Incrementing = Feature Response From Incrementing Values In This Column By One FR_delta\n",
      "     Explanation: Represents how much the predicted output values respond to adding one FR_delta amount to every value in this column\n",
      "FRD_MAD = Feature Response From Decrementing- Median Absolute Delta\n",
      "     Explanation: Takes the absolute value of all changes in predictions, then takes the median of those. Useful for seeing if decrementing this feature provokes strong changes that are both positive and negative\n",
      "FRI_MAD = Feature Response From Incrementing- Median Absolute Delta\n",
      "     Explanation: Takes the absolute value of all changes in predictions, then takes the median of those. Useful for seeing if incrementing this feature provokes strong changes that are both positive and negative\n",
      "FRD_abs = Feature Response From Decrementing Avg Absolute Change\n",
      "     Explanation: What is the average absolute change in predicted output values to subtracting one FR_delta amount to every value in this column. Useful for seeing if output is sensitive to a feature, but not in a uniformly positive or negative way\n",
      "FRI_abs = Feature Response From Incrementing Avg Absolute Change\n",
      "     Explanation: What is the average absolute change in predicted output values to adding one FR_delta amount to every value in this column. Useful for seeing if output is sensitive to a feature, but not in a uniformly positive or negative way\n",
      "*******\n",
      "\n",
      "Here is our brier-score-loss, which is the default value we optimized for while training, and is the value returned from .score() unless you requested a custom scoring metric\n",
      "It is a measure of how close the PROBABILITY predictions are.\n",
      "0.0272\n",
      "\n",
      "Here is the trained estimator's overall accuracy (when it predicts a label, how frequently is that the correct label?)\n",
      "97.1%\n",
      "\n",
      "Here is a confusion matrix showing predictions vs. actuals by label:\n",
      "Predicted >    0   1  All\n",
      "v Actual v               \n",
      "0            110   0  110\n",
      "1              5  56   61\n",
      "All          115  56  171\n",
      "\n",
      "Here is predictive value by class:\n",
      "Class:  1 = 1.0\n",
      "Class:  0 = 0.9565217391304348\n",
      "+--------------------------------------+-----------------------------------+--------------------------------+\n",
      "| Bucket Edges                         |   Predicted Probability Of Bucket |   Actual Probability of Bucket |\n",
      "|--------------------------------------+-----------------------------------+--------------------------------|\n",
      "| (-3.064500000000001e-06, 8.5231e-06] |                            0.0000 |                         0.0000 |\n",
      "| (8.5231e-06, 1.0015e-05]             |                            0.0000 |                         0.0000 |\n",
      "| (1.0015e-05, 1.6427e-05]             |                            0.0000 |                         0.0000 |\n",
      "| (1.6427e-05, 1.6891e-05]             |                            0.0000 |                         0.0000 |\n",
      "| (1.6891e-05, 9.1022e-05]             |                            0.0000 |                         0.0588 |\n",
      "| (9.1022e-05, 0.0013662]              |                            0.0004 |                         0.0588 |\n",
      "| (0.0013662, 0.94777]                 |                            0.3398 |                         0.4706 |\n",
      "| (0.94777, 0.99995]                   |                            0.9923 |                         1.0000 |\n",
      "| (0.99995, 0.99996]                   |                            1.0000 |                         1.0000 |\n",
      "| (0.99996, 0.99999]                   |                            1.0000 |                         1.0000 |\n",
      "+--------------------------------------+-----------------------------------+--------------------------------+\n",
      "\n",
      "Here is the accuracy of our trained estimator at each level of predicted probabilities\n",
      "For a verbose description of what this means, please visit the docs:\n",
      "http://auto-ml.readthedocs.io/en/latest/analytics.html#interpreting-predicted-probability-buckets-for-classifiers\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#aml = data.drop(cell_3 + cell_2, axis = 1)\n",
    "#aml = (X + y)\n",
    "\n",
    "aml_train, aml_test = train_test_split(data, test_size=0.3, random_state=11)\n",
    "\n",
    "column_descriptions = {\n",
    "    'Target': 'output',\n",
    "    'ID': 'ignore'\n",
    "}\n",
    "\n",
    "ml_predictor = Predictor(type_of_estimator='classifier', column_descriptions=column_descriptions)\n",
    "\n",
    "ml_predictor.train(aml_train)\n",
    "\n",
    "score = ml_predictor.score(aml_test, aml_test.Target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      0.14710\n",
      "1      0.07017\n",
      "2      0.12790\n",
      "3      0.10520\n",
      "4      0.10430\n",
      "        ...   \n",
      "564    0.13890\n",
      "565    0.09791\n",
      "566    0.05302\n",
      "567    0.15200\n",
      "568    0.00000\n",
      "Name: Concave Points-1, Length: 569, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(data['Concave Points-1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
