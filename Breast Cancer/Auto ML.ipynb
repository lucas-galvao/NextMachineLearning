{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliotecas e Base de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucas/anaconda3/lib/python3.7/site-packages/sklearn/externals/six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from auto_ml import Predictor\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = ['ID', 'Target']\n",
    "cell_1 = ['Radius-1', 'Texture-1', 'Perimeter-1', 'Area-1', 'Smoothness-1', 'Compactness-1', 'Concavity-1', 'Concave Points-1', 'Symmetry-1', 'Fractal Dimension-1']\n",
    "cell_2 = ['Radius-2', 'Texture-2', 'Perimeter-2', 'Area-2', 'Smoothness-2', 'Compactness-2', 'Concavity-2', 'Concave Points-2', 'Symmetry-2', 'Fractal Dimension-2']\n",
    "cell_3 = ['Radius-3', 'Texture-3', 'Perimeter-3', 'Area-3', 'Smoothness-3', 'Compactness-3', 'Concavity-3', 'Concave Points-3', 'Symmetry-3', 'Fractal Dimension-3']\n",
    "\n",
    "header = head + cell_1 + cell_2 + cell_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('wdbc.data', names=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Target'] = data['Target'].map({'M': 1, 'B': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data.drop(['ID'] + cell_3 + cell_2, axis = 1)\n",
    "data2 = data.drop(['ID'] + cell_3, axis = 1)\n",
    "data3 = data.drop(['ID'], axis = 1)\n",
    "y = data['Target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Célula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to auto_ml! We're about to go through and make sense of your data using machine learning, and give you a production-ready pipeline to get predictions with.\n",
      "\n",
      "If you have any issues, or new feature ideas, let us know at http://auto.ml\n",
      "You are running on version 2.9.10\n",
      "Now using the model training_params that you passed in:\n",
      "{}\n",
      "After overwriting our defaults with your values, here are the final params that will be used to initialize the model:\n",
      "{'presort': False, 'learning_rate': 0.1, 'warm_start': True}\n",
      "Running basic data cleaning\n",
      "Fitting DataFrameVectorizer\n",
      "Now using the model training_params that you passed in:\n",
      "{}\n",
      "After overwriting our defaults with your values, here are the final params that will be used to initialize the model:\n",
      "{'presort': False, 'learning_rate': 0.1, 'warm_start': True}\n",
      "\n",
      "\n",
      "********************************************************************************************\n",
      "About to fit the pipeline for the model GradientBoostingClassifier to predict Target\n",
      "Started at:\n",
      "2020-01-28 17:08:00\n",
      "[1] random_holdout_set_from_training_data's score is: -0.198\n",
      "[2] random_holdout_set_from_training_data's score is: -0.168\n",
      "[3] random_holdout_set_from_training_data's score is: -0.144\n",
      "[4] random_holdout_set_from_training_data's score is: -0.124\n",
      "[5] random_holdout_set_from_training_data's score is: -0.109\n",
      "[6] random_holdout_set_from_training_data's score is: -0.094\n",
      "[7] random_holdout_set_from_training_data's score is: -0.083\n",
      "[8] random_holdout_set_from_training_data's score is: -0.072\n",
      "[9] random_holdout_set_from_training_data's score is: -0.064\n",
      "[10] random_holdout_set_from_training_data's score is: -0.058\n",
      "[11] random_holdout_set_from_training_data's score is: -0.052\n",
      "[12] random_holdout_set_from_training_data's score is: -0.047\n",
      "[13] random_holdout_set_from_training_data's score is: -0.045\n",
      "[14] random_holdout_set_from_training_data's score is: -0.042\n",
      "[15] random_holdout_set_from_training_data's score is: -0.038\n",
      "[16] random_holdout_set_from_training_data's score is: -0.036\n",
      "[17] random_holdout_set_from_training_data's score is: -0.035\n",
      "[18] random_holdout_set_from_training_data's score is: -0.034\n",
      "[19] random_holdout_set_from_training_data's score is: -0.032\n",
      "[20] random_holdout_set_from_training_data's score is: -0.031\n",
      "[21] random_holdout_set_from_training_data's score is: -0.031\n",
      "[22] random_holdout_set_from_training_data's score is: -0.031\n",
      "[23] random_holdout_set_from_training_data's score is: -0.03\n",
      "[24] random_holdout_set_from_training_data's score is: -0.028\n",
      "[25] random_holdout_set_from_training_data's score is: -0.027\n",
      "[26] random_holdout_set_from_training_data's score is: -0.027\n",
      "[27] random_holdout_set_from_training_data's score is: -0.026\n",
      "[28] random_holdout_set_from_training_data's score is: -0.025\n",
      "[29] random_holdout_set_from_training_data's score is: -0.025\n",
      "[30] random_holdout_set_from_training_data's score is: -0.026\n",
      "[31] random_holdout_set_from_training_data's score is: -0.025\n",
      "[32] random_holdout_set_from_training_data's score is: -0.025\n",
      "[33] random_holdout_set_from_training_data's score is: -0.024\n",
      "[34] random_holdout_set_from_training_data's score is: -0.024\n",
      "[35] random_holdout_set_from_training_data's score is: -0.024\n",
      "[36] random_holdout_set_from_training_data's score is: -0.024\n",
      "[37] random_holdout_set_from_training_data's score is: -0.023\n",
      "[38] random_holdout_set_from_training_data's score is: -0.023\n",
      "[39] random_holdout_set_from_training_data's score is: -0.024\n",
      "[40] random_holdout_set_from_training_data's score is: -0.024\n",
      "[41] random_holdout_set_from_training_data's score is: -0.023\n",
      "[42] random_holdout_set_from_training_data's score is: -0.023\n",
      "[43] random_holdout_set_from_training_data's score is: -0.022\n",
      "[44] random_holdout_set_from_training_data's score is: -0.022\n",
      "[45] random_holdout_set_from_training_data's score is: -0.022\n",
      "[46] random_holdout_set_from_training_data's score is: -0.022\n",
      "[47] random_holdout_set_from_training_data's score is: -0.021\n",
      "[48] random_holdout_set_from_training_data's score is: -0.021\n",
      "[49] random_holdout_set_from_training_data's score is: -0.021\n",
      "[50] random_holdout_set_from_training_data's score is: -0.021\n",
      "[52] random_holdout_set_from_training_data's score is: -0.021\n",
      "[54] random_holdout_set_from_training_data's score is: -0.021\n",
      "[56] random_holdout_set_from_training_data's score is: -0.022\n",
      "[58] random_holdout_set_from_training_data's score is: -0.021\n",
      "[60] random_holdout_set_from_training_data's score is: -0.021\n",
      "[62] random_holdout_set_from_training_data's score is: -0.022\n",
      "[64] random_holdout_set_from_training_data's score is: -0.022\n",
      "[66] random_holdout_set_from_training_data's score is: -0.021\n",
      "[68] random_holdout_set_from_training_data's score is: -0.02\n",
      "[70] random_holdout_set_from_training_data's score is: -0.02\n",
      "[72] random_holdout_set_from_training_data's score is: -0.019\n",
      "[74] random_holdout_set_from_training_data's score is: -0.019\n",
      "[76] random_holdout_set_from_training_data's score is: -0.019\n",
      "[78] random_holdout_set_from_training_data's score is: -0.018\n",
      "[80] random_holdout_set_from_training_data's score is: -0.018\n",
      "[82] random_holdout_set_from_training_data's score is: -0.018\n",
      "[84] random_holdout_set_from_training_data's score is: -0.018\n",
      "[86] random_holdout_set_from_training_data's score is: -0.018\n",
      "[88] random_holdout_set_from_training_data's score is: -0.018\n",
      "[90] random_holdout_set_from_training_data's score is: -0.018\n",
      "[92] random_holdout_set_from_training_data's score is: -0.018\n",
      "[94] random_holdout_set_from_training_data's score is: -0.018\n",
      "[96] random_holdout_set_from_training_data's score is: -0.018\n",
      "[98] random_holdout_set_from_training_data's score is: -0.018\n",
      "[100] random_holdout_set_from_training_data's score is: -0.018\n",
      "[103] random_holdout_set_from_training_data's score is: -0.018\n",
      "[106] random_holdout_set_from_training_data's score is: -0.018\n",
      "[109] random_holdout_set_from_training_data's score is: -0.018\n",
      "[112] random_holdout_set_from_training_data's score is: -0.018\n",
      "[115] random_holdout_set_from_training_data's score is: -0.018\n",
      "[118] random_holdout_set_from_training_data's score is: -0.018\n",
      "[121] random_holdout_set_from_training_data's score is: -0.019\n",
      "[124] random_holdout_set_from_training_data's score is: -0.018\n",
      "[127] random_holdout_set_from_training_data's score is: -0.019\n",
      "[130] random_holdout_set_from_training_data's score is: -0.019\n",
      "[133] random_holdout_set_from_training_data's score is: -0.019\n",
      "[136] random_holdout_set_from_training_data's score is: -0.019\n",
      "[139] random_holdout_set_from_training_data's score is: -0.018\n",
      "[142] random_holdout_set_from_training_data's score is: -0.018\n",
      "[145] random_holdout_set_from_training_data's score is: -0.018\n",
      "[148] random_holdout_set_from_training_data's score is: -0.019\n",
      "[151] random_holdout_set_from_training_data's score is: -0.019\n",
      "[154] random_holdout_set_from_training_data's score is: -0.018\n",
      "[157] random_holdout_set_from_training_data's score is: -0.018\n",
      "[160] random_holdout_set_from_training_data's score is: -0.018\n",
      "The number of estimators that were the best for this training dataset: 100\n",
      "The best score on the holdout set: -0.017502149033923164\n",
      "Finished training the pipeline!\n",
      "Total training time:\n",
      "0:00:01\n",
      "\n",
      "\n",
      "Here are the results from our GradientBoostingClassifier\n",
      "predicting Target\n",
      "Calculating feature responses, for advanced analytics.\n",
      "The printed list will only contain at most the top 100 features.\n",
      "+----+---------------------+--------------+----------+-------------------+-------------------+-----------+-----------+-----------+-----------+\n",
      "|    | Feature Name        |   Importance |    Delta |   FR_Decrementing |   FR_Incrementing |   FRD_abs |   FRI_abs |   FRD_MAD |   FRI_MAD |\n",
      "|----+---------------------+--------------+----------+-------------------+-------------------+-----------+-----------+-----------+-----------|\n",
      "|  9 | Fractal Dimension-1 |       0.0031 |   0.0037 |           -0.0001 |           -0.0008 |    0.0017 |    0.0014 |    0.0000 |    0.0000 |\n",
      "|  8 | Symmetry-1          |       0.0055 |   0.0136 |           -0.0019 |            0.0009 |    0.0027 |    0.0021 |    0.0000 |    0.0000 |\n",
      "|  5 | Compactness-1       |       0.0058 |   0.0276 |            0.0005 |           -0.0002 |    0.0037 |    0.0024 |    0.0000 |    0.0000 |\n",
      "|  0 | Radius-1            |       0.0217 |   1.7961 |            0.0141 |           -0.0132 |    0.0162 |    0.0146 |    0.0000 |    0.0000 |\n",
      "|  4 | Smoothness-1        |       0.0238 |   0.0071 |           -0.0037 |            0.0028 |    0.0037 |    0.0029 |    0.0000 |    0.0000 |\n",
      "|  3 | Area-1              |       0.0641 | 178.6978 |           -0.0426 |            0.1552 |    0.0426 |    0.1552 |    0.0000 |    0.0064 |\n",
      "|  6 | Concavity-1         |       0.0684 |   0.0407 |           -0.0167 |            0.0201 |    0.0189 |    0.0216 |    0.0000 |    0.0001 |\n",
      "|  1 | Texture-1           |       0.1010 |   2.1463 |           -0.0238 |            0.0179 |    0.0247 |    0.0209 |    0.0006 |    0.0006 |\n",
      "|  7 | Concave Points-1    |       0.2604 |   0.0197 |           -0.0211 |            0.0243 |    0.0215 |    0.0249 |    0.0000 |    0.0004 |\n",
      "|  2 | Perimeter-1         |       0.4461 |  12.3711 |           -0.0002 |            0.0131 |    0.0059 |    0.0135 |    0.0000 |    0.0002 |\n",
      "+----+---------------------+--------------+----------+-------------------+-------------------+-----------+-----------+-----------+-----------+\n",
      "\n",
      "\n",
      "*******\n",
      "Legend:\n",
      "Importance = Feature Importance\n",
      "     Explanation: A weighted measure of how much of the variance the model is able to explain is due to this column\n",
      "FR_delta = Feature Response Delta Amount\n",
      "     Explanation: Amount this column was incremented or decremented by to calculate the feature reponses\n",
      "FR_Decrementing = Feature Response From Decrementing Values In This Column By One FR_delta\n",
      "     Explanation: Represents how much the predicted output values respond to subtracting one FR_delta amount from every value in this column\n",
      "FR_Incrementing = Feature Response From Incrementing Values In This Column By One FR_delta\n",
      "     Explanation: Represents how much the predicted output values respond to adding one FR_delta amount to every value in this column\n",
      "FRD_MAD = Feature Response From Decrementing- Median Absolute Delta\n",
      "     Explanation: Takes the absolute value of all changes in predictions, then takes the median of those. Useful for seeing if decrementing this feature provokes strong changes that are both positive and negative\n",
      "FRI_MAD = Feature Response From Incrementing- Median Absolute Delta\n",
      "     Explanation: Takes the absolute value of all changes in predictions, then takes the median of those. Useful for seeing if incrementing this feature provokes strong changes that are both positive and negative\n",
      "FRD_abs = Feature Response From Decrementing Avg Absolute Change\n",
      "     Explanation: What is the average absolute change in predicted output values to subtracting one FR_delta amount to every value in this column. Useful for seeing if output is sensitive to a feature, but not in a uniformly positive or negative way\n",
      "FRI_abs = Feature Response From Incrementing Avg Absolute Change\n",
      "     Explanation: What is the average absolute change in predicted output values to adding one FR_delta amount to every value in this column. Useful for seeing if output is sensitive to a feature, but not in a uniformly positive or negative way\n",
      "*******\n",
      "\n",
      "Here is our brier-score-loss, which is the default value we optimized for while training, and is the value returned from .score() unless you requested a custom scoring metric\n",
      "It is a measure of how close the PROBABILITY predictions are.\n",
      "0.0345\n",
      "\n",
      "Here is the trained estimator's overall accuracy (when it predicts a label, how frequently is that the correct label?)\n",
      "95.3%\n",
      "\n",
      "Here is a confusion matrix showing predictions vs. actuals by label:\n",
      "Predicted >    0   1  All\n",
      "v Actual v               \n",
      "0            107   3  110\n",
      "1              5  56   61\n",
      "All          112  59  171\n",
      "\n",
      "Here is predictive value by class:\n",
      "Class:  1 = 0.9491525423728814\n",
      "Class:  0 = 0.9553571428571429\n",
      "+-------------------------------------+-----------------------------------+--------------------------------+\n",
      "| Bucket Edges                        |   Predicted Probability Of Bucket |   Actual Probability of Bucket |\n",
      "|-------------------------------------+-----------------------------------+--------------------------------|\n",
      "| (-0.00040800000000000005, 0.000865] |                            0.0007 |                         0.0000 |\n",
      "| (0.000865, 0.00107]                 |                            0.0010 |                         0.0000 |\n",
      "| (0.00107, 0.00162]                  |                            0.0014 |                         0.0588 |\n",
      "| (0.00162, 0.0024]                   |                            0.0019 |                         0.0000 |\n",
      "| (0.0024, 0.00533]                   |                            0.0038 |                         0.0588 |\n",
      "| (0.00533, 0.073]                    |                            0.0272 |                         0.0000 |\n",
      "| (0.073, 0.871]                      |                            0.4169 |                         0.5294 |\n",
      "| (0.871, 0.997]                      |                            0.9692 |                         0.9412 |\n",
      "| (0.997, 0.998]                      |                            0.9980 |                         1.0000 |\n",
      "| (0.998, 1.0]                        |                            0.9987 |                         1.0000 |\n",
      "+-------------------------------------+-----------------------------------+--------------------------------+\n",
      "\n",
      "Here is the accuracy of our trained estimator at each level of predicted probabilities\n",
      "For a verbose description of what this means, please visit the docs:\n",
      "http://auto-ml.readthedocs.io/en/latest/analytics.html#interpreting-predicted-probability-buckets-for-classifiers\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "aml_train, aml_test = train_test_split(data1, test_size=0.3, random_state=11)\n",
    "\n",
    "column_descriptions = {\n",
    "    'Target': 'output',\n",
    "    'ID': 'ignore'\n",
    "}\n",
    "\n",
    "ml_predictor = Predictor(type_of_estimator='classifier', column_descriptions=column_descriptions)\n",
    "\n",
    "ml_predictor.train(aml_train)\n",
    "\n",
    "score = ml_predictor.score(aml_test, aml_test.Target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Células"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to auto_ml! We're about to go through and make sense of your data using machine learning, and give you a production-ready pipeline to get predictions with.\n",
      "\n",
      "If you have any issues, or new feature ideas, let us know at http://auto.ml\n",
      "You are running on version 2.9.10\n",
      "Now using the model training_params that you passed in:\n",
      "{}\n",
      "After overwriting our defaults with your values, here are the final params that will be used to initialize the model:\n",
      "{'presort': False, 'learning_rate': 0.1, 'warm_start': True}\n",
      "Running basic data cleaning\n",
      "Fitting DataFrameVectorizer\n",
      "Now using the model training_params that you passed in:\n",
      "{}\n",
      "After overwriting our defaults with your values, here are the final params that will be used to initialize the model:\n",
      "{'presort': False, 'learning_rate': 0.1, 'warm_start': True}\n",
      "\n",
      "\n",
      "********************************************************************************************\n",
      "About to fit the pipeline for the model GradientBoostingClassifier to predict Target\n",
      "Started at:\n",
      "2020-01-28 17:08:01\n",
      "[1] random_holdout_set_from_training_data's score is: -0.197\n",
      "[2] random_holdout_set_from_training_data's score is: -0.165\n",
      "[3] random_holdout_set_from_training_data's score is: -0.139\n",
      "[4] random_holdout_set_from_training_data's score is: -0.122\n",
      "[5] random_holdout_set_from_training_data's score is: -0.108\n",
      "[6] random_holdout_set_from_training_data's score is: -0.093\n",
      "[7] random_holdout_set_from_training_data's score is: -0.082\n",
      "[8] random_holdout_set_from_training_data's score is: -0.074\n",
      "[9] random_holdout_set_from_training_data's score is: -0.065\n",
      "[10] random_holdout_set_from_training_data's score is: -0.06\n",
      "[11] random_holdout_set_from_training_data's score is: -0.055\n",
      "[12] random_holdout_set_from_training_data's score is: -0.051\n",
      "[13] random_holdout_set_from_training_data's score is: -0.047\n",
      "[14] random_holdout_set_from_training_data's score is: -0.044\n",
      "[15] random_holdout_set_from_training_data's score is: -0.04\n",
      "[16] random_holdout_set_from_training_data's score is: -0.036\n",
      "[17] random_holdout_set_from_training_data's score is: -0.034\n",
      "[18] random_holdout_set_from_training_data's score is: -0.032\n",
      "[19] random_holdout_set_from_training_data's score is: -0.03\n",
      "[20] random_holdout_set_from_training_data's score is: -0.029\n",
      "[21] random_holdout_set_from_training_data's score is: -0.028\n",
      "[22] random_holdout_set_from_training_data's score is: -0.027\n",
      "[23] random_holdout_set_from_training_data's score is: -0.027\n",
      "[24] random_holdout_set_from_training_data's score is: -0.025\n",
      "[25] random_holdout_set_from_training_data's score is: -0.024\n",
      "[26] random_holdout_set_from_training_data's score is: -0.023\n",
      "[27] random_holdout_set_from_training_data's score is: -0.022\n",
      "[28] random_holdout_set_from_training_data's score is: -0.021\n",
      "[29] random_holdout_set_from_training_data's score is: -0.021\n",
      "[30] random_holdout_set_from_training_data's score is: -0.021\n",
      "[31] random_holdout_set_from_training_data's score is: -0.02\n",
      "[32] random_holdout_set_from_training_data's score is: -0.02\n",
      "[33] random_holdout_set_from_training_data's score is: -0.021\n",
      "[34] random_holdout_set_from_training_data's score is: -0.021\n",
      "[35] random_holdout_set_from_training_data's score is: -0.02\n",
      "[36] random_holdout_set_from_training_data's score is: -0.02\n",
      "[37] random_holdout_set_from_training_data's score is: -0.02\n",
      "[38] random_holdout_set_from_training_data's score is: -0.021\n",
      "[39] random_holdout_set_from_training_data's score is: -0.019\n",
      "[40] random_holdout_set_from_training_data's score is: -0.019\n",
      "[41] random_holdout_set_from_training_data's score is: -0.02\n",
      "[42] random_holdout_set_from_training_data's score is: -0.018\n",
      "[43] random_holdout_set_from_training_data's score is: -0.018\n",
      "[44] random_holdout_set_from_training_data's score is: -0.018\n",
      "[45] random_holdout_set_from_training_data's score is: -0.018\n",
      "[46] random_holdout_set_from_training_data's score is: -0.017\n",
      "[47] random_holdout_set_from_training_data's score is: -0.017\n",
      "[48] random_holdout_set_from_training_data's score is: -0.018\n",
      "[49] random_holdout_set_from_training_data's score is: -0.019\n",
      "[50] random_holdout_set_from_training_data's score is: -0.018\n",
      "[52] random_holdout_set_from_training_data's score is: -0.018\n",
      "[54] random_holdout_set_from_training_data's score is: -0.017\n",
      "[56] random_holdout_set_from_training_data's score is: -0.016\n",
      "[58] random_holdout_set_from_training_data's score is: -0.015\n",
      "[60] random_holdout_set_from_training_data's score is: -0.014\n",
      "[62] random_holdout_set_from_training_data's score is: -0.014\n",
      "[64] random_holdout_set_from_training_data's score is: -0.014\n",
      "[66] random_holdout_set_from_training_data's score is: -0.014\n",
      "[68] random_holdout_set_from_training_data's score is: -0.013\n",
      "[70] random_holdout_set_from_training_data's score is: -0.013\n",
      "[72] random_holdout_set_from_training_data's score is: -0.014\n",
      "[74] random_holdout_set_from_training_data's score is: -0.014\n",
      "[76] random_holdout_set_from_training_data's score is: -0.015\n",
      "[78] random_holdout_set_from_training_data's score is: -0.015\n",
      "[80] random_holdout_set_from_training_data's score is: -0.014\n",
      "[82] random_holdout_set_from_training_data's score is: -0.014\n",
      "[84] random_holdout_set_from_training_data's score is: -0.013\n",
      "[86] random_holdout_set_from_training_data's score is: -0.012\n",
      "[88] random_holdout_set_from_training_data's score is: -0.011\n",
      "[90] random_holdout_set_from_training_data's score is: -0.01\n",
      "[92] random_holdout_set_from_training_data's score is: -0.011\n",
      "[94] random_holdout_set_from_training_data's score is: -0.011\n",
      "[96] random_holdout_set_from_training_data's score is: -0.011\n",
      "[98] random_holdout_set_from_training_data's score is: -0.011\n",
      "[100] random_holdout_set_from_training_data's score is: -0.011\n",
      "[103] random_holdout_set_from_training_data's score is: -0.011\n",
      "[106] random_holdout_set_from_training_data's score is: -0.01\n",
      "[109] random_holdout_set_from_training_data's score is: -0.01\n",
      "[112] random_holdout_set_from_training_data's score is: -0.011\n",
      "[115] random_holdout_set_from_training_data's score is: -0.013\n",
      "[118] random_holdout_set_from_training_data's score is: -0.012\n",
      "[121] random_holdout_set_from_training_data's score is: -0.012\n",
      "[124] random_holdout_set_from_training_data's score is: -0.012\n",
      "[127] random_holdout_set_from_training_data's score is: -0.011\n",
      "[130] random_holdout_set_from_training_data's score is: -0.012\n",
      "[133] random_holdout_set_from_training_data's score is: -0.013\n",
      "[136] random_holdout_set_from_training_data's score is: -0.014\n",
      "[139] random_holdout_set_from_training_data's score is: -0.013\n",
      "[142] random_holdout_set_from_training_data's score is: -0.013\n",
      "[145] random_holdout_set_from_training_data's score is: -0.014\n",
      "[148] random_holdout_set_from_training_data's score is: -0.014\n",
      "[151] random_holdout_set_from_training_data's score is: -0.014\n",
      "[154] random_holdout_set_from_training_data's score is: -0.014\n",
      "[157] random_holdout_set_from_training_data's score is: -0.014\n",
      "[160] random_holdout_set_from_training_data's score is: -0.014\n",
      "[163] random_holdout_set_from_training_data's score is: -0.015\n",
      "[166] random_holdout_set_from_training_data's score is: -0.016\n",
      "The number of estimators that were the best for this training dataset: 106\n",
      "The best score on the holdout set: -0.009984784931030645\n",
      "Finished training the pipeline!\n",
      "Total training time:\n",
      "0:00:02\n",
      "\n",
      "\n",
      "Here are the results from our GradientBoostingClassifier\n",
      "predicting Target\n",
      "Calculating feature responses, for advanced analytics.\n",
      "The printed list will only contain at most the top 100 features.\n",
      "+----+---------------------+--------------+----------+-------------------+-------------------+-----------+-----------+-----------+-----------+\n",
      "|    | Feature Name        |   Importance |    Delta |   FR_Decrementing |   FR_Incrementing |   FRD_abs |   FRI_abs |   FRD_MAD |   FRI_MAD |\n",
      "|----+---------------------+--------------+----------+-------------------+-------------------+-----------+-----------+-----------+-----------|\n",
      "|  9 | Fractal Dimension-1 |       0.0006 |   0.0037 |           -0.0001 |            0.0000 |    0.0001 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 10 | Radius-2            |       0.0017 |   0.1412 |           -0.0013 |            0.0008 |    0.0013 |    0.0008 |    0.0000 |    0.0000 |\n",
      "|  5 | Compactness-1       |       0.0020 |   0.0276 |           -0.0002 |            0.0005 |    0.0008 |    0.0010 |    0.0000 |    0.0000 |\n",
      "| 15 | Compactness-2       |       0.0030 |   0.0094 |            0.0006 |           -0.0013 |    0.0009 |    0.0014 |    0.0000 |    0.0000 |\n",
      "|  8 | Symmetry-1          |       0.0030 |   0.0136 |           -0.0008 |            0.0001 |    0.0010 |    0.0004 |    0.0000 |    0.0000 |\n",
      "| 14 | Smoothness-2        |       0.0031 |   0.0016 |            0.0026 |           -0.0008 |    0.0027 |    0.0008 |    0.0000 |    0.0000 |\n",
      "| 18 | Symmetry-2          |       0.0048 |   0.0042 |            0.0009 |           -0.0010 |    0.0009 |    0.0011 |    0.0000 |    0.0000 |\n",
      "| 17 | Concave Points-2    |       0.0062 |   0.0032 |            0.0008 |           -0.0017 |    0.0008 |    0.0019 |    0.0000 |    0.0000 |\n",
      "| 11 | Texture-2           |       0.0066 |   0.2792 |            0.0022 |           -0.0015 |    0.0022 |    0.0016 |    0.0000 |    0.0000 |\n",
      "| 16 | Concavity-2         |       0.0068 |   0.0165 |            0.0007 |           -0.0002 |    0.0008 |    0.0005 |    0.0000 |    0.0000 |\n",
      "| 12 | Perimeter-2         |       0.0068 |   1.0428 |           -0.0012 |            0.0005 |    0.0021 |    0.0010 |    0.0000 |    0.0000 |\n",
      "| 19 | Fractal Dimension-2 |       0.0081 |   0.0014 |            0.0026 |           -0.0021 |    0.0027 |    0.0023 |    0.0000 |    0.0000 |\n",
      "|  0 | Radius-1            |       0.0081 |   1.7961 |            0.0044 |           -0.0030 |    0.0045 |    0.0033 |    0.0000 |    0.0000 |\n",
      "|  4 | Smoothness-1        |       0.0122 |   0.0071 |           -0.0008 |            0.0001 |    0.0012 |    0.0009 |    0.0000 |    0.0000 |\n",
      "| 13 | Area-2              |       0.0275 |  22.1091 |           -0.0106 |            0.0138 |    0.0107 |    0.0141 |    0.0000 |    0.0018 |\n",
      "|  6 | Concavity-1         |       0.0359 |   0.0407 |           -0.0140 |            0.0120 |    0.0160 |    0.0124 |    0.0000 |    0.0005 |\n",
      "|  3 | Area-1              |       0.0614 | 178.6978 |           -0.0252 |            0.0619 |    0.0253 |    0.0619 |    0.0000 |    0.0009 |\n",
      "|  1 | Texture-1           |       0.0859 |   2.1463 |           -0.0145 |            0.0133 |    0.0155 |    0.0134 |    0.0002 |    0.0003 |\n",
      "|  2 | Perimeter-1         |       0.1595 |  12.3711 |           -0.0023 |            0.0087 |    0.0030 |    0.0094 |    0.0000 |    0.0000 |\n",
      "|  7 | Concave Points-1    |       0.5571 |   0.0197 |           -0.0266 |            0.0199 |    0.0273 |    0.0211 |    0.0000 |    0.0001 |\n",
      "+----+---------------------+--------------+----------+-------------------+-------------------+-----------+-----------+-----------+-----------+\n",
      "\n",
      "\n",
      "*******\n",
      "Legend:\n",
      "Importance = Feature Importance\n",
      "     Explanation: A weighted measure of how much of the variance the model is able to explain is due to this column\n",
      "FR_delta = Feature Response Delta Amount\n",
      "     Explanation: Amount this column was incremented or decremented by to calculate the feature reponses\n",
      "FR_Decrementing = Feature Response From Decrementing Values In This Column By One FR_delta\n",
      "     Explanation: Represents how much the predicted output values respond to subtracting one FR_delta amount from every value in this column\n",
      "FR_Incrementing = Feature Response From Incrementing Values In This Column By One FR_delta\n",
      "     Explanation: Represents how much the predicted output values respond to adding one FR_delta amount to every value in this column\n",
      "FRD_MAD = Feature Response From Decrementing- Median Absolute Delta\n",
      "     Explanation: Takes the absolute value of all changes in predictions, then takes the median of those. Useful for seeing if decrementing this feature provokes strong changes that are both positive and negative\n",
      "FRI_MAD = Feature Response From Incrementing- Median Absolute Delta\n",
      "     Explanation: Takes the absolute value of all changes in predictions, then takes the median of those. Useful for seeing if incrementing this feature provokes strong changes that are both positive and negative\n",
      "FRD_abs = Feature Response From Decrementing Avg Absolute Change\n",
      "     Explanation: What is the average absolute change in predicted output values to subtracting one FR_delta amount to every value in this column. Useful for seeing if output is sensitive to a feature, but not in a uniformly positive or negative way\n",
      "FRI_abs = Feature Response From Incrementing Avg Absolute Change\n",
      "     Explanation: What is the average absolute change in predicted output values to adding one FR_delta amount to every value in this column. Useful for seeing if output is sensitive to a feature, but not in a uniformly positive or negative way\n",
      "*******\n",
      "\n",
      "Here is our brier-score-loss, which is the default value we optimized for while training, and is the value returned from .score() unless you requested a custom scoring metric\n",
      "It is a measure of how close the PROBABILITY predictions are.\n",
      "0.0330\n",
      "\n",
      "Here is the trained estimator's overall accuracy (when it predicts a label, how frequently is that the correct label?)\n",
      "94.7%\n",
      "\n",
      "Here is a confusion matrix showing predictions vs. actuals by label:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted >    0   1  All\n",
      "v Actual v               \n",
      "0            109   1  110\n",
      "1              8  53   61\n",
      "All          117  54  171\n",
      "\n",
      "Here is predictive value by class:\n",
      "Class:  1 = 0.9814814814814815\n",
      "Class:  0 = 0.9316239316239316\n",
      "+----------------------+-----------------------------------+--------------------------------+\n",
      "| Bucket Edges         |   Predicted Probability Of Bucket |   Actual Probability of Bucket |\n",
      "|----------------------+-----------------------------------+--------------------------------|\n",
      "| (-0.00057, 0.000598] |                            0.0005 |                         0.0000 |\n",
      "| (0.000598, 0.00079]  |                            0.0007 |                         0.0000 |\n",
      "| (0.00079, 0.00113]   |                            0.0009 |                         0.0000 |\n",
      "| (0.00113, 0.0023]    |                            0.0016 |                         0.0588 |\n",
      "| (0.0023, 0.00399]    |                            0.0033 |                         0.0000 |\n",
      "| (0.00399, 0.0394]    |                            0.0176 |                         0.0588 |\n",
      "| (0.0394, 0.765]      |                            0.2788 |                         0.4706 |\n",
      "| (0.765, 0.996]       |                            0.9431 |                         1.0000 |\n",
      "| (0.996, 0.998]       |                            0.9979 |                         1.0000 |\n",
      "| (0.998, 0.999]       |                            0.9986 |                         1.0000 |\n",
      "+----------------------+-----------------------------------+--------------------------------+\n",
      "\n",
      "Here is the accuracy of our trained estimator at each level of predicted probabilities\n",
      "For a verbose description of what this means, please visit the docs:\n",
      "http://auto-ml.readthedocs.io/en/latest/analytics.html#interpreting-predicted-probability-buckets-for-classifiers\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "aml_train, aml_test = train_test_split(data2, test_size=0.3, random_state=11)\n",
    "\n",
    "column_descriptions = {\n",
    "    'Target': 'output',\n",
    "    'ID': 'ignore'\n",
    "}\n",
    "\n",
    "ml_predictor = Predictor(type_of_estimator='classifier', column_descriptions=column_descriptions)\n",
    "\n",
    "ml_predictor.train(aml_train)\n",
    "\n",
    "score = ml_predictor.score(aml_test, aml_test.Target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Células"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to auto_ml! We're about to go through and make sense of your data using machine learning, and give you a production-ready pipeline to get predictions with.\n",
      "\n",
      "If you have any issues, or new feature ideas, let us know at http://auto.ml\n",
      "You are running on version 2.9.10\n",
      "Now using the model training_params that you passed in:\n",
      "{}\n",
      "After overwriting our defaults with your values, here are the final params that will be used to initialize the model:\n",
      "{'presort': False, 'learning_rate': 0.1, 'warm_start': True}\n",
      "Running basic data cleaning\n",
      "Fitting DataFrameVectorizer\n",
      "Now using the model training_params that you passed in:\n",
      "{}\n",
      "After overwriting our defaults with your values, here are the final params that will be used to initialize the model:\n",
      "{'presort': False, 'learning_rate': 0.1, 'warm_start': True}\n",
      "\n",
      "\n",
      "********************************************************************************************\n",
      "About to fit the pipeline for the model GradientBoostingClassifier to predict Target\n",
      "Started at:\n",
      "2020-01-28 17:08:03\n",
      "[1] random_holdout_set_from_training_data's score is: -0.174\n",
      "[2] random_holdout_set_from_training_data's score is: -0.146\n",
      "[3] random_holdout_set_from_training_data's score is: -0.122\n",
      "[4] random_holdout_set_from_training_data's score is: -0.104\n",
      "[5] random_holdout_set_from_training_data's score is: -0.086\n",
      "[6] random_holdout_set_from_training_data's score is: -0.072\n",
      "[7] random_holdout_set_from_training_data's score is: -0.061\n",
      "[8] random_holdout_set_from_training_data's score is: -0.053\n",
      "[9] random_holdout_set_from_training_data's score is: -0.046\n",
      "[10] random_holdout_set_from_training_data's score is: -0.04\n",
      "[11] random_holdout_set_from_training_data's score is: -0.037\n",
      "[12] random_holdout_set_from_training_data's score is: -0.033\n",
      "[13] random_holdout_set_from_training_data's score is: -0.029\n",
      "[14] random_holdout_set_from_training_data's score is: -0.026\n",
      "[15] random_holdout_set_from_training_data's score is: -0.024\n",
      "[16] random_holdout_set_from_training_data's score is: -0.022\n",
      "[17] random_holdout_set_from_training_data's score is: -0.02\n",
      "[18] random_holdout_set_from_training_data's score is: -0.019\n",
      "[19] random_holdout_set_from_training_data's score is: -0.018\n",
      "[20] random_holdout_set_from_training_data's score is: -0.016\n",
      "[21] random_holdout_set_from_training_data's score is: -0.016\n",
      "[22] random_holdout_set_from_training_data's score is: -0.015\n",
      "[23] random_holdout_set_from_training_data's score is: -0.014\n",
      "[24] random_holdout_set_from_training_data's score is: -0.013\n",
      "[25] random_holdout_set_from_training_data's score is: -0.014\n",
      "[26] random_holdout_set_from_training_data's score is: -0.012\n",
      "[27] random_holdout_set_from_training_data's score is: -0.013\n",
      "[28] random_holdout_set_from_training_data's score is: -0.013\n",
      "[29] random_holdout_set_from_training_data's score is: -0.013\n",
      "[30] random_holdout_set_from_training_data's score is: -0.013\n",
      "[31] random_holdout_set_from_training_data's score is: -0.014\n",
      "[32] random_holdout_set_from_training_data's score is: -0.013\n",
      "[33] random_holdout_set_from_training_data's score is: -0.014\n",
      "[34] random_holdout_set_from_training_data's score is: -0.014\n",
      "[35] random_holdout_set_from_training_data's score is: -0.013\n",
      "[36] random_holdout_set_from_training_data's score is: -0.013\n",
      "[37] random_holdout_set_from_training_data's score is: -0.012\n",
      "[38] random_holdout_set_from_training_data's score is: -0.012\n",
      "[39] random_holdout_set_from_training_data's score is: -0.013\n",
      "[40] random_holdout_set_from_training_data's score is: -0.012\n",
      "[41] random_holdout_set_from_training_data's score is: -0.011\n",
      "[42] random_holdout_set_from_training_data's score is: -0.011\n",
      "[43] random_holdout_set_from_training_data's score is: -0.011\n",
      "[44] random_holdout_set_from_training_data's score is: -0.011\n",
      "[45] random_holdout_set_from_training_data's score is: -0.011\n",
      "[46] random_holdout_set_from_training_data's score is: -0.011\n",
      "[47] random_holdout_set_from_training_data's score is: -0.011\n",
      "[48] random_holdout_set_from_training_data's score is: -0.011\n",
      "[49] random_holdout_set_from_training_data's score is: -0.01\n",
      "[50] random_holdout_set_from_training_data's score is: -0.011\n",
      "[52] random_holdout_set_from_training_data's score is: -0.011\n",
      "[54] random_holdout_set_from_training_data's score is: -0.012\n",
      "[56] random_holdout_set_from_training_data's score is: -0.011\n",
      "[58] random_holdout_set_from_training_data's score is: -0.011\n",
      "[60] random_holdout_set_from_training_data's score is: -0.011\n",
      "[62] random_holdout_set_from_training_data's score is: -0.01\n",
      "[64] random_holdout_set_from_training_data's score is: -0.01\n",
      "[66] random_holdout_set_from_training_data's score is: -0.009\n",
      "[68] random_holdout_set_from_training_data's score is: -0.009\n",
      "[70] random_holdout_set_from_training_data's score is: -0.009\n",
      "[72] random_holdout_set_from_training_data's score is: -0.008\n",
      "[74] random_holdout_set_from_training_data's score is: -0.009\n",
      "[76] random_holdout_set_from_training_data's score is: -0.009\n",
      "[78] random_holdout_set_from_training_data's score is: -0.008\n",
      "[80] random_holdout_set_from_training_data's score is: -0.008\n",
      "[82] random_holdout_set_from_training_data's score is: -0.008\n",
      "[84] random_holdout_set_from_training_data's score is: -0.008\n",
      "[86] random_holdout_set_from_training_data's score is: -0.008\n",
      "[88] random_holdout_set_from_training_data's score is: -0.008\n",
      "[90] random_holdout_set_from_training_data's score is: -0.008\n",
      "[92] random_holdout_set_from_training_data's score is: -0.008\n",
      "[94] random_holdout_set_from_training_data's score is: -0.008\n",
      "[96] random_holdout_set_from_training_data's score is: -0.007\n",
      "[98] random_holdout_set_from_training_data's score is: -0.008\n",
      "[100] random_holdout_set_from_training_data's score is: -0.008\n",
      "[103] random_holdout_set_from_training_data's score is: -0.008\n",
      "[106] random_holdout_set_from_training_data's score is: -0.007\n",
      "[109] random_holdout_set_from_training_data's score is: -0.008\n",
      "[112] random_holdout_set_from_training_data's score is: -0.007\n",
      "[115] random_holdout_set_from_training_data's score is: -0.007\n",
      "[118] random_holdout_set_from_training_data's score is: -0.006\n",
      "[121] random_holdout_set_from_training_data's score is: -0.006\n",
      "[124] random_holdout_set_from_training_data's score is: -0.006\n",
      "[127] random_holdout_set_from_training_data's score is: -0.006\n",
      "[130] random_holdout_set_from_training_data's score is: -0.006\n",
      "[133] random_holdout_set_from_training_data's score is: -0.006\n",
      "[136] random_holdout_set_from_training_data's score is: -0.006\n",
      "[139] random_holdout_set_from_training_data's score is: -0.006\n",
      "[142] random_holdout_set_from_training_data's score is: -0.006\n",
      "[145] random_holdout_set_from_training_data's score is: -0.006\n",
      "[148] random_holdout_set_from_training_data's score is: -0.006\n",
      "[151] random_holdout_set_from_training_data's score is: -0.005\n",
      "[154] random_holdout_set_from_training_data's score is: -0.006\n",
      "[157] random_holdout_set_from_training_data's score is: -0.005\n",
      "[160] random_holdout_set_from_training_data's score is: -0.005\n",
      "[163] random_holdout_set_from_training_data's score is: -0.005\n",
      "[166] random_holdout_set_from_training_data's score is: -0.005\n",
      "[169] random_holdout_set_from_training_data's score is: -0.005\n",
      "[172] random_holdout_set_from_training_data's score is: -0.005\n",
      "[175] random_holdout_set_from_training_data's score is: -0.005\n",
      "[178] random_holdout_set_from_training_data's score is: -0.005\n",
      "[181] random_holdout_set_from_training_data's score is: -0.005\n",
      "[184] random_holdout_set_from_training_data's score is: -0.005\n",
      "[187] random_holdout_set_from_training_data's score is: -0.005\n",
      "[190] random_holdout_set_from_training_data's score is: -0.005\n",
      "[193] random_holdout_set_from_training_data's score is: -0.005\n",
      "[196] random_holdout_set_from_training_data's score is: -0.005\n",
      "[199] random_holdout_set_from_training_data's score is: -0.005\n",
      "[202] random_holdout_set_from_training_data's score is: -0.005\n",
      "[205] random_holdout_set_from_training_data's score is: -0.005\n",
      "[208] random_holdout_set_from_training_data's score is: -0.005\n",
      "[211] random_holdout_set_from_training_data's score is: -0.005\n",
      "The number of estimators that were the best for this training dataset: 151\n",
      "The best score on the holdout set: -0.00533032925826509\n",
      "Finished training the pipeline!\n",
      "Total training time:\n",
      "0:00:02\n",
      "\n",
      "\n",
      "Here are the results from our GradientBoostingClassifier\n",
      "predicting Target\n",
      "Calculating feature responses, for advanced analytics.\n",
      "The printed list will only contain at most the top 100 features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------------+--------------+----------+-------------------+-------------------+-----------+-----------+-----------+-----------+\n",
      "|    | Feature Name        |   Importance |    Delta |   FR_Decrementing |   FR_Incrementing |   FRD_abs |   FRI_abs |   FRD_MAD |   FRI_MAD |\n",
      "|----+---------------------+--------------+----------+-------------------+-------------------+-----------+-----------+-----------+-----------|\n",
      "| 15 | Compactness-2       |       0.0000 |   0.0094 |            0.0000 |           -0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 10 | Radius-2            |       0.0000 |   0.1412 |           -0.0000 |            0.0001 |    0.0000 |    0.0001 |    0.0000 |    0.0000 |\n",
      "| 25 | Compactness-3       |       0.0000 |   0.0803 |           -0.0002 |            0.0000 |    0.0002 |    0.0000 |    0.0000 |    0.0000 |\n",
      "|  4 | Smoothness-1        |       0.0001 |   0.0071 |           -0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 19 | Fractal Dimension-2 |       0.0002 |   0.0014 |            0.0005 |           -0.0001 |    0.0005 |    0.0001 |    0.0000 |    0.0000 |\n",
      "| 18 | Symmetry-2          |       0.0003 |   0.0042 |            0.0001 |           -0.0001 |    0.0001 |    0.0001 |    0.0000 |    0.0000 |\n",
      "| 12 | Perimeter-2         |       0.0003 |   1.0428 |           -0.0000 |            0.0001 |    0.0001 |    0.0001 |    0.0000 |    0.0000 |\n",
      "|  8 | Symmetry-1          |       0.0005 |   0.0136 |           -0.0005 |            0.0001 |    0.0005 |    0.0001 |    0.0000 |    0.0000 |\n",
      "|  9 | Fractal Dimension-1 |       0.0006 |   0.0037 |            0.0001 |           -0.0000 |    0.0001 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 28 | Symmetry-3          |       0.0007 |   0.0313 |           -0.0001 |            0.0002 |    0.0001 |    0.0002 |    0.0000 |    0.0000 |\n",
      "|  2 | Perimeter-1         |       0.0009 |  12.3711 |            0.0000 |           -0.0001 |    0.0001 |    0.0003 |    0.0000 |    0.0000 |\n",
      "| 29 | Fractal Dimension-3 |       0.0010 |   0.0090 |           -0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 16 | Concavity-2         |       0.0012 |   0.0165 |           -0.0001 |            0.0000 |    0.0001 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 17 | Concave Points-2    |       0.0018 |   0.0032 |            0.0000 |           -0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "|  5 | Compactness-1       |       0.0026 |   0.0276 |            0.0001 |           -0.0000 |    0.0001 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 14 | Smoothness-2        |       0.0030 |   0.0016 |            0.0003 |           -0.0000 |    0.0005 |    0.0000 |    0.0000 |    0.0000 |\n",
      "|  6 | Concavity-1         |       0.0037 |   0.0407 |           -0.0005 |           -0.0000 |    0.0006 |    0.0001 |    0.0000 |    0.0000 |\n",
      "|  0 | Radius-1            |       0.0048 |   1.7961 |           -0.0002 |            0.0022 |    0.0003 |    0.0022 |    0.0000 |    0.0000 |\n",
      "|  3 | Area-1              |       0.0050 | 178.6978 |            0.0015 |            0.0009 |    0.0017 |    0.0010 |    0.0000 |    0.0000 |\n",
      "| 11 | Texture-2           |       0.0061 |   0.2792 |           -0.0006 |           -0.0000 |    0.0006 |    0.0001 |    0.0000 |    0.0000 |\n",
      "|  7 | Concave Points-1    |       0.0089 |   0.0197 |           -0.0004 |            0.0014 |    0.0005 |    0.0015 |    0.0000 |    0.0000 |\n",
      "| 13 | Area-2              |       0.0097 |  22.1091 |           -0.0011 |            0.0006 |    0.0011 |    0.0006 |    0.0000 |    0.0000 |\n",
      "| 20 | Radius-3            |       0.0100 |   2.4589 |           -0.0016 |            0.0039 |    0.0016 |    0.0040 |    0.0000 |    0.0000 |\n",
      "| 24 | Smoothness-3        |       0.0117 |   0.0114 |           -0.0008 |            0.0002 |    0.0008 |    0.0002 |    0.0000 |    0.0000 |\n",
      "| 26 | Concavity-3         |       0.0227 |   0.1061 |           -0.0011 |            0.0007 |    0.0011 |    0.0007 |    0.0000 |    0.0000 |\n",
      "|  1 | Texture-1           |       0.0288 |   2.1463 |           -0.0012 |            0.0011 |    0.0012 |    0.0011 |    0.0000 |    0.0000 |\n",
      "| 21 | Texture-3           |       0.0455 |   3.0455 |           -0.0014 |            0.0017 |    0.0014 |    0.0017 |    0.0000 |    0.0000 |\n",
      "| 27 | Concave Points-3    |       0.1217 |   0.0331 |           -0.0054 |            0.0049 |    0.0055 |    0.0049 |    0.0000 |    0.0000 |\n",
      "| 23 | Area-3              |       0.2356 | 287.7963 |           -0.0142 |            0.0306 |    0.0150 |    0.0306 |    0.0000 |    0.0001 |\n",
      "| 22 | Perimeter-3         |       0.4727 |  17.1585 |           -0.0014 |            0.0098 |    0.0015 |    0.0098 |    0.0000 |    0.0000 |\n",
      "+----+---------------------+--------------+----------+-------------------+-------------------+-----------+-----------+-----------+-----------+\n",
      "\n",
      "\n",
      "*******\n",
      "Legend:\n",
      "Importance = Feature Importance\n",
      "     Explanation: A weighted measure of how much of the variance the model is able to explain is due to this column\n",
      "FR_delta = Feature Response Delta Amount\n",
      "     Explanation: Amount this column was incremented or decremented by to calculate the feature reponses\n",
      "FR_Decrementing = Feature Response From Decrementing Values In This Column By One FR_delta\n",
      "     Explanation: Represents how much the predicted output values respond to subtracting one FR_delta amount from every value in this column\n",
      "FR_Incrementing = Feature Response From Incrementing Values In This Column By One FR_delta\n",
      "     Explanation: Represents how much the predicted output values respond to adding one FR_delta amount to every value in this column\n",
      "FRD_MAD = Feature Response From Decrementing- Median Absolute Delta\n",
      "     Explanation: Takes the absolute value of all changes in predictions, then takes the median of those. Useful for seeing if decrementing this feature provokes strong changes that are both positive and negative\n",
      "FRI_MAD = Feature Response From Incrementing- Median Absolute Delta\n",
      "     Explanation: Takes the absolute value of all changes in predictions, then takes the median of those. Useful for seeing if incrementing this feature provokes strong changes that are both positive and negative\n",
      "FRD_abs = Feature Response From Decrementing Avg Absolute Change\n",
      "     Explanation: What is the average absolute change in predicted output values to subtracting one FR_delta amount to every value in this column. Useful for seeing if output is sensitive to a feature, but not in a uniformly positive or negative way\n",
      "FRI_abs = Feature Response From Incrementing Avg Absolute Change\n",
      "     Explanation: What is the average absolute change in predicted output values to adding one FR_delta amount to every value in this column. Useful for seeing if output is sensitive to a feature, but not in a uniformly positive or negative way\n",
      "*******\n",
      "\n",
      "Here is our brier-score-loss, which is the default value we optimized for while training, and is the value returned from .score() unless you requested a custom scoring metric\n",
      "It is a measure of how close the PROBABILITY predictions are.\n",
      "0.0254\n",
      "\n",
      "Here is the trained estimator's overall accuracy (when it predicts a label, how frequently is that the correct label?)\n",
      "96.5%\n",
      "\n",
      "Here is a confusion matrix showing predictions vs. actuals by label:\n",
      "Predicted >    0   1  All\n",
      "v Actual v               \n",
      "0            108   2  110\n",
      "1              4  57   61\n",
      "All          112  59  171\n",
      "\n",
      "Here is predictive value by class:\n",
      "Class:  1 = 0.9661016949152542\n",
      "Class:  0 = 0.9642857142857143\n",
      "+-------------------------------------+-----------------------------------+--------------------------------+\n",
      "| Bucket Edges                        |   Predicted Probability Of Bucket |   Actual Probability of Bucket |\n",
      "|-------------------------------------+-----------------------------------+--------------------------------|\n",
      "| (-3.003200000000001e-06, 1.024e-05] |                            0.0000 |                         0.0000 |\n",
      "| (1.024e-05, 1.2451e-05]             |                            0.0000 |                         0.0000 |\n",
      "| (1.2451e-05, 1.3397e-05]            |                            0.0000 |                         0.0000 |\n",
      "| (1.3397e-05, 1.8155e-05]            |                            0.0000 |                         0.0000 |\n",
      "| (1.8155e-05, 7.3827e-05]            |                            0.0000 |                         0.0588 |\n",
      "| (7.3827e-05, 0.0009096]             |                            0.0003 |                         0.0588 |\n",
      "| (0.0009096, 0.94695]                |                            0.4733 |                         0.4706 |\n",
      "| (0.94695, 0.99994]                  |                            0.9966 |                         1.0000 |\n",
      "| (0.99994, 0.99997]                  |                            1.0000 |                         1.0000 |\n",
      "| (0.99997, 0.99999]                  |                            1.0000 |                         1.0000 |\n",
      "+-------------------------------------+-----------------------------------+--------------------------------+\n",
      "\n",
      "Here is the accuracy of our trained estimator at each level of predicted probabilities\n",
      "For a verbose description of what this means, please visit the docs:\n",
      "http://auto-ml.readthedocs.io/en/latest/analytics.html#interpreting-predicted-probability-buckets-for-classifiers\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "aml_train, aml_test = train_test_split(data3, test_size=0.3, random_state=11)\n",
    "\n",
    "column_descriptions = {\n",
    "    'Target': 'output',\n",
    "    'ID': 'ignore'\n",
    "}\n",
    "\n",
    "ml_predictor = Predictor(type_of_estimator='classifier', column_descriptions=column_descriptions)\n",
    "\n",
    "ml_predictor.train(aml_train)\n",
    "\n",
    "score = ml_predictor.score(aml_test, aml_test.Target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
